{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<span style=\"color: rgb(0,53,91);\">\n",
    "<center><img src=\"./Imagenes/ITESO_Logo.png\" style=\"width:500px;height:142px;\" title=\"Logo ITESO\"></center>\n",
    "<font face = \"Times New Roman\" size = \"6\"><b><center>Maestría en Sistemas Computacionales</center></b></font>\n",
    "<font face = \"Times New Roman\" size = \"5\"><b><center>Programación para Análisis de Datos</center></b></font>\n",
    "\n",
    "<b><br><font face = \"Times New Roman\" size = \"4\"><center>Unidad 5: Proceso de Selección de Métodos</center></font>\n",
    "<font face = \"Times New Roman\" size = \"4\"><center>Tema 5.2: Redes Neuronales</center></font>\n",
    "<font face = \"Times New Roman\" size = \"4\"><center>Subtema d: Red Neuronal con TensorFlow</center></font></b>\n",
    "<div align=\"right\"><font face = \"Times New Roman\" size = \"2\">Dr. Iván Esteban Villalón Turrubiates (villalon@iteso.mx)</font></div>\n",
    "</span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RED NEURONAL CON TENSORFLOW\n",
    "\n",
    "#### Primer Implementación con TensorFlow\n",
    "Para este ejemplo se emplearán datos para un clasificador de rangos de precios de celulares (*mobile price range*). El **DataSet** de ***entrenamiento*** consiste de 21 columnas y 2000 registros, los cuales fueron obtenidos del portal Kaggle.\n",
    "\n",
    "<center><img src=\"./Imagenes/Kaggle_logo.png\" style=\"width:240px;height:86px;\" class=\"center\"></center>\n",
    "\n",
    "Las columnas de información contienen los siguientes datos:\n",
    "\n",
    "* **battery_power**: Total de energía que la batería puede almacenar [mAh].\n",
    "* **blue**: Soporte para Bluetooth [0,1]. \n",
    "* **clock_speed**: Velocidad del microprocesador [GHz].\n",
    "* **dual_sim**: Soporte para tarjeta SIM dual [0,1].\n",
    "* **fc**: Cantidad de megapixeles de la cámara frontal [MP].\n",
    "* **four_g**: Soporte para red 4G [0,1].\n",
    "* **int_memory**: Cantidad de memoria interna [GB]. \n",
    "* **m_dep**: Profundidad móvil [cm].\n",
    "* **mobile_wt**: Peso del dispositivo [gr].\n",
    "* **n_cores**: Número de núcleos del procesador [1..8].\n",
    "* **pc**: Cantidad de megapixeles de la cámara principal [MP].\n",
    "* **px_height**: Alto de la resolución [pixeles].\n",
    "* **px_width**: Ancho de la resolución [pixeles].\n",
    "* **ram**: Cantidad de memoria del dispositivo [MB]. \n",
    "* **sc_h**: Alto de la pantalla [cm].\n",
    "* **sc_w**: Ancho de la pantalla [cm].\n",
    "* **talk_time**: Tiempo de llamada con batería completa [hr].\n",
    "* **three_g**: Soporte para red 3G [0,1].\n",
    "* **touch_screen**: Soporte para pantalla táctil [0,1].\n",
    "* **wifi** Soporte para acceso a red inalámbrica WiFi [0,1].\n",
    "* **price_range** Rango de precio del dispositivo [0 a 3].\n",
    "\n",
    "Los datos completos del **DataSet** se pueden consultar a través [de esta liga](https://www.kaggle.com/iabhishekofficial/mobile-price-classification).\n",
    "\n",
    "**El problema consiste en predecir el rango de precio en el que un celular se encuentra dependiendo de sus características.**\n",
    "\n",
    "Los rangos de precio están divididos en 4 clases (0, 1, 2 y 3).\n",
    "\n",
    "Como primer paso, se carga el archivo `.CSV` que contiene el **DataSet** de ***entrenamiento*** llamado `train.csv` empleando un **DataFrame** de **Pandas**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importación de librerías\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Definición de los parámetros de los gráficos\n",
    "plt.rcParams.update({'font.size': 11, 'figure.figsize': (8, 6)}) \n",
    "\n",
    "#Lectura del DataSet de entrenamiento\n",
    "data_df = pd.read_csv('./Datos/train.csv')\n",
    "\n",
    "#Impresión de los Resultados\n",
    "data_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, los datos se convertirán de un **DataFrame** de **Pandas** a un arreglo de **NumPy**, con el objetivo de tener los datos de entrada y salida para la **Red Neuronal**. Esto es:\n",
    "\n",
    "* Datos de entrada (X): Todas las columnas del **DataFrame** excepto la última (`price_range`).\n",
    "* Datos de salida (y): Sólamente la última columna del **DataFrame** (`price_range`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cambio de un DataFrame de Pandas a un arreglo de Numpy\n",
    "X = data_df.iloc[:,:20].values\n",
    "y = data_df.iloc[:,20:21].values\n",
    "\n",
    "print(\"El tamaño del arreglo de entrada (X) es:\", X.shape)\n",
    "print(\"El tamaño del arreglo de salida (y) es:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se realizarán algunas operaciones de preparación de los datos. Para ello se empleará la librería **SciKit-Learn**, la cual contiene herramientas simples y eficientes para análisis predictivo de datos.\n",
    "\n",
    "<center><img src=\"./Imagenes/scikit.png\" style=\"width:200px;height:108px;\" class=\"center\"></center>\n",
    "\n",
    "La documentación de **SciKit-Learn** se puede encontrar a través [de esta liga](https://scikit-learn.org/stable/).\n",
    "\n",
    "Los pasos a seguir son:\n",
    "1. Normalización de los datos de entrada a través del método `StandardScaler()`, el cual realiza la transformación de los datos de manera que su distribución tendrá un valor de media igual a 0 y desviación estándar igual a 1, y lo realiza de manera independiente para cada columna del arreglo de **NumPy**. \n",
    "\n",
    "2. Categorización de los datos de salida a través del método `OneHotEncoder()`, el cual es una representación de valores categóricos como vectores binarios, donde un 0 representa que no existe y un 1 representa que existe.\n",
    "\n",
    "3. Extracción de arreglos de entrenamiento y prueba de manera aleatoria empleando el método `train_test_split()`, por medio del cual se general estos arreglos para la entrada (X) y la salida (y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importación de librerías\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Normalización de los Datos de Entrada\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "print('Datos de entrada (X) Normalizados: \\n', X[0])\n",
    "\n",
    "#Categorización de los Datos de Salida\n",
    "ohe = OneHotEncoder(categories='auto')\n",
    "y = ohe.fit_transform(y).toarray()\n",
    "print('\\nDatos de salida (y) Categorizados: \\n', y[0:10])\n",
    "\n",
    "#Extracción de arreglos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 0)\n",
    "\n",
    "#Revisión de los tamaños de los arreglos\n",
    "print(\"\\nEl tamaño del arreglo de entrada normalizado (X) es:\", X.shape)\n",
    "print(\"El tamaño del arreglo de salida categorizado (y) es:\", y.shape)\n",
    "print(\"El tamaño del arreglo de entrada (X) para entrenamiento es:\", X_train.shape)\n",
    "print(\"El tamaño del arreglo de entrada (X) para prueba es:\", X_test.shape)\n",
    "print(\"El tamaño del arreglo de salida (y) para entrenamiento es:\", y_train.shape)\n",
    "print(\"El tamaño del arreglo de salida (y) para prueba es:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se prepara una **Red Neuronal** basada en el modelo secuencial (`Sequential`) de **Keras** a través de la librería **TensorFlow**, el cual consiste en una apilación (*stack*) lineal de capas (*layers*), esto es, se puede crear un modelo secuencial pasando la lista de capas a través del método `.add()`.\n",
    "\n",
    "En este caso, tendremos tres capas (*layers*) basadas en el modelo secuencial (`Sequential`) de **Keras** a través de la librería **TensorFlow** que consisten en:\n",
    "\n",
    "1. Primer capa del tipo `Dense`, la cual es una capa del tipo ***Totalmente Conectada (Fully Connected)*** con vector de salida 16-dimensional. Debido a que es la primera capa, se especifica la forma de los datos de entrada, en este caso, son vectores 20-dimensionales. Adicionalmente, se emplea una función de activación del tipo `relu`.\n",
    "2. La siguiente capa del tipo `Dense` tiene un vector de salida 12-dimensional y una función de activación del tipo `relu`.\n",
    "3. La siguiente capa del tipo `Dense` tiene un vector de salida 4-dimensional y una función de activación del tipo `softmax`.\n",
    "\n",
    "Las funciones de activación disponibles en **TensorFlow** se pueden consultar [en esta liga](https://www.tensorflow.org/api_docs/python/tf/keras/activations).\n",
    "\n",
    "\n",
    "> ***Nota:*** Una de las partes más importantes de **TensorFlow** es que debe ser rápido su proceso de ejecución. Con una instalación adecuada de la librería, debe funcionar haciendo uso de CPUs, GPUs o TPUs. Parte del proceso de ejecución emplea distinto código dependiendo del hardware disponible, por ejemplo, algunos CPUs pueden realizar operaciones que otros no pueden, como la adición vectorizada (agregación de múltiples variables al mismo tiempo).\n",
    "> \n",
    "> Debido a ello, es posible que **TensorFlow** genere un ***warning*** al ejecutarse con un mensaje similar a `This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations`, pero es solo para indicar que la versión instalada puede emplear las operaciones AVX (Advanced Vector Extensions) de manera predeterminada en ciertos tipos de situaciones (por ejemplo en las multiplicaciones de matrices en el *forward-propagation* o en el *back-propagation*) con el objetivo de mejorar la velocidad de procesamiento. ***Esto no es un error, simplemente indica que podría hacer uso de las ventajas del CPU disponible para mejorar el proceso***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importación de la Librería TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "#Preparación del Modelo\n",
    "model = tf.keras.Sequential()\n",
    "model.add(Dense(16, input_dim = 20, activation = 'relu'))\n",
    "model.add(Dense(12, activation = 'relu'))\n",
    "model.add(Dense(4, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede visualizar una representación del modelo de la **Red Neuronal** creada con **TensorFlow**. Para ello se emplea el método `.summary()` aplicado al modelo. Esto es:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualización de la Red Neuronal\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se configurará el modelo para su entrenamiento, empleando el método `.compile()` de **Keras** aplicado al modelo de la **Red Neuronal**. Para ello:\n",
    "\n",
    "* Se define el nombre de la función objetivo para el proceso de optimización (loss) por medio de una cadena, en este caso es `loss='categorical_crossentropy'`.\n",
    "* Se define la instancia que define el método a emplear para el proceso de optimización, en este caso es `optimizer='adam'`. \n",
    "* Se define la lista de las métricas que se evaluarán por el modelo durante el proceso de entrenamiento y prueba, en este caso es `metrics=['accuracy']`.\n",
    "\n",
    "La descripción detallada del método `.compile()` se puede consultar [en esta liga](https://keras.io/models/model/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configuración del modelo de entrenamiento\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se inicializará el **TensorBoard**, que es una herramienta que proporciona mediciones y visualizaciones para el flujo de trabajo del modelo, lo cual ayuda a realizar un seguimiento de métricas como pérdida y precisión, visualización de gráficos de modelos, incrustación de proyectos en espacios de dimensiones inferiores, entre otras cosas. Para emplear **TensorBoard** se realizan estos pasos:\n",
    "\n",
    "* Se importa la librería **TensorBoard** desde **TensorFlow**.\n",
    "* Se define el directorio de registro para **TensorBoard**. \n",
    "* Se crea el llamado (callback) de eventos para **TensorBoard**.\n",
    "\n",
    "***Nota:*** Más adelante se detallará el uso de **TensorBoard**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importación de la Librería TensorBoard\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "#Definir el directorio de registro para TensorBoard\n",
    "log_dir = \"./logs/fit/\"\n",
    "\n",
    "#Crear el llamado (callback) de eventos para TensorBoard\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se realiza el entrenamiento por un cierto número de épocas (*epochs*), las cuales son las iteraciones que se realizan al **DataSet**. Para ello se emplea el método `.fit()` de **Keras** por medio de los siguientes parámetros:\n",
    "\n",
    "* Se proporcionan los datos de entrada de entrenamiento (*X_train*).\n",
    "* Se proporcionan los datos de salida de entrenamiento (*y_train*).\n",
    "* Se definen los valores de validación por medio de los datos de prueba.\n",
    "* Se define la fraccion de los datos de entenamiento que se emplearán como datos de validación, en este caso se usa el 25% de ellos (`validation_split = 0.25`).\n",
    "* Se define la cantidad de épocas a emplear (`epochs = 100`).\n",
    "* Se define el número de muestras que se emplearán para actualizar el gradiente (`batch_size = 64`).\n",
    "* Se define el *callback* para el registro que empleará **TensorBoard**.\n",
    "* Se indica la manera como se muestran los resultados (`verbose = 1`), en este caso se muestra cada época por medio de una barra de progreso.\n",
    "\n",
    "La descripción detallada del método `.fit()` se puede consultar [en esta liga](https://keras.io/models/model/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Entrenamiento del modelo\n",
    "history = model.fit(x = X_train, \n",
    "                    y = y_train, \n",
    "                    validation_data=(X_test, y_test),\n",
    "                    validation_split = 0.25, \n",
    "                    epochs = 100, \n",
    "                    batch_size = 64, \n",
    "                    callbacks=[tensorboard_callback],\n",
    "                    verbose = 1)\n",
    "#history = model.fit(X_train, y_train, validation_split = 0.25, epochs = 100, batch_size = 64, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como siguiente paso se generan las predicciones de salida desde las muestras de prueba (*test*) de la entrada. Para ello se emplea el método `.predict()` de **Keras**. Para ello:\n",
    "\n",
    "* Se proporcionan los datos de entrada de prueba (*X_test*).\n",
    "\n",
    "La descripción detallada del método `.predict()` se puede consultar [en esta liga](https://keras.io/models/model/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicciones de la salida en base a los datos de prueba de entrada\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#Conversión de las predicciones a etiquetas\n",
    "pred = list()\n",
    "for i in range(len(y_pred)):\n",
    "    pred.append(np.argmax(y_pred[i]))\n",
    "print('Primeras predicciones de la Salida: \\n', pred[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se realiza la conversión de la categorización de prueba de los datos (*One Hot Encoded*) a etiquetas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conversión de \"hot encoded test label\" a \"label\"\n",
    "test = list()\n",
    "for i in range(len(y_test)):\n",
    "    test.append(np.argmax(y_test[i]))\n",
    "print('Primeras etiquetas de la Salida: \\n', test[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se determina el nivel de certeza o asertividad (*accuracy*) del resultado comparando la predicción de la salida (`pred`) con la salida de prueba (`test`). Para ello se emplea la función `accuracy_score()` de la librería **SciKit-Learn**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importación de librerías\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Se determina el nivel de asertividad (accuracy)\n",
    "a = accuracy_score(pred,test)\n",
    "print('El nivel de certeza (accuracy) es:', a*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se muestra el gráfico del nivel de asertividad (*accuracy*) de los **DataSet** de entrenamiento y validación a través de las épocas (*epoch*) de entenamiento, con lo cual se puede realizar una comparativa de su desempeño.\n",
    "\n",
    "La documentación de **Keras** recomienda emplear el código mostrado a continuación, el cual puede ser consultado [en esta liga](https://keras.io/visualization/#training-history-visualization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resumen del entrenamiento por medio de la asertividad (accuracy)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Nivel de Certeza del Modelo')\n",
    "plt.ylabel('Certeza')\n",
    "plt.xlabel('Épocas')\n",
    "plt.legend(['Entrenamiento', 'Prueba'], loc='best')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se muestra el gráfico del nivel de pérdida (*loss*) de los **DataSet** de entrenamiento y validación a través de las épocas (*epoch*) de entenamiento, con lo cual se puede realizar una comparativa de su desempeño.\n",
    "\n",
    "La documentación de **Keras** recomienda emplear el código mostrado a continuación, el cual puede ser consultado [en esta liga](https://keras.io/visualization/#training-history-visualization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resumen del entrenamiento por medio de la pérdida (loss)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Nivel de Pérdida del Modelo')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.xlabel('Épocas')\n",
    "plt.legend(['Entrenamiento', 'Prueba'], loc='best')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicción de un Valor en Particular\n",
    "\n",
    "Para generar una predicción del costo en base a una serie de valores de un teléfono celular en particular, se considerarán los siguientes valores:\n",
    "\n",
    ">1. **battery_power**: Total de energía que la batería puede almacenar: ***1500 mAh***.\n",
    ">2. **blue**: Soporte para Bluetooth: ***Si (1)***. \n",
    ">3. **clock_speed**: Velocidad del microprocesador: ***2.5 GHz***.\n",
    ">4. **dual_sim**: Soporte para tarjeta SIM dual: ***No (0)***.\n",
    ">5. **fc**: Cantidad de megapixeles de la cámara frontal: ***10 MP***.\n",
    ">6. **four_g**: Soporte para red 4G: ***Si (1)***.\n",
    ">7. **int_memory**: Cantidad de memoria interna: ***256 GB***. \n",
    ">8. **m_dep**: Profundidad móvil: ***1 cm***.\n",
    ">9. **mobile_wt**: Peso del dispositivo: ***200 gr***.\n",
    ">10. **n_cores**: Número de núcleos del procesador: ***8***.\n",
    ">11. **pc**: Cantidad de megapixeles de la cámara principal: ***15 MP***.\n",
    ">12. **px_height**: Alto de la resolución: ***1880 pixeles***.\n",
    ">13. **px_width**: Ancho de la resolución: ***1495 pixeles***.\n",
    ">14. **ram**: Cantidad de memoria del dispositivo: ***64 MB***. \n",
    ">15. **sc_h**: Alto de la pantalla: ***15 cm***.\n",
    ">16. **sc_w**: Ancho de la pantalla: ***6 cm***.\n",
    ">17. **talk_time**: Tiempo de llamada con batería completa: ***20 hr***.\n",
    ">18. **three_g**: Soporte para red 3G: ***Si (1)***.\n",
    ">19. **touch_screen**: Soporte para pantalla táctil: ***Si (1)***.\n",
    ">20. **wifi** Soporte para acceso a red inalámbrica WiFi: ***Si (1)***.\n",
    "\n",
    "El procedimiento para realizar la predicción con base en el modelo entrenado (**model**) se realiza por medio de los siguientes pasos:\n",
    "\n",
    "1. Se define un arreglo bidimensional de **NumPy** que contenga los 20 valores de las características del celular de interés, para ello se emplea la función `np.array([[]])`.\n",
    "2. Se realiza la Normalización de los datos de entrada a través del método `StandardScaler()`, el cual realiza la transformación de los datos de manera que su distribución tendrá un valor de media igual a 0 y desviación estándar igual a 1 en los datos del arreglo de **NumPy**. \n",
    "3. Se genera la predicción de la salida (el costo del teléfono celular) empleando el arreglo de **NumPy** como los datos de entrada, empleando el método `.predict()` de **Keras** aplicado al modelo entrenado (**model**).\n",
    "4. Se realiza la conversión del resultado de una categorización (*One Hot Encoded*) a la etiqueta final.\n",
    "5. Se muestra el resultado.\n",
    "\n",
    "Esto es:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definición del Arreglo con los Valores\n",
    "arreglo = np.array([[1500, 1,  2.5, 0, 10, 1, 256, 1, 200, 8, 15, 1880, 1495, 64, 15, 6, 20, 1, 1, 1]])\n",
    "\n",
    "#Normalización de los Datos de Entrada\n",
    "arreglo = scaler.fit_transform(arreglo)\n",
    "\n",
    "#Predicciones de la Salida en base a los datos\n",
    "mo_pred = model.predict(arreglo)\n",
    "\n",
    "#Conversión de las Predicciones a Etiquetas\n",
    "predic = list()\n",
    "for i in range(len(mo_pred)):\n",
    "    predic.append(np.argmax(mo_pred[i]))\n",
    "\n",
    "#Impresión de los Resultados    \n",
    "print(\"La predicción del costo para las caraterísticas indicadas es: Categoría número\", predic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso de Tensorboard\n",
    "\n",
    "En el proceso del **Aprendizaje Automático** es necesario mejorar los resultados que se han obtenido, para ello se requiere realizar mediciones adecuadas de los rendimientos del modelo. **TensorBoard** es una herramienta para proporcionar las medidas y visualizaciones necesarias durante el flujo de trabajo de **Aprendizaje Automático**, ya que permite rastrear métricas de experimentos como pérdida y precisión, visualizar el gráfico del modelo, proyectar incrustaciones en un espacio dimensional inferior, y mucho más.\n",
    "\n",
    "**TensorBoard** proporciona la visualización y las herramientas necesarias para experimentar con elementos como:\n",
    "- Seguir y visualizar métricas tales como la pérdida y la exactitud.\n",
    "- Visualizar el grafo del modelo (operaciones y capas).\n",
    "- Ver histogramas de pesos, sesgos y otros tensores a medida que cambian con el tiempo.\n",
    "- Proyectar incorporaciones en un espacio de dimensiones más bajas.\n",
    "- Mostrar imágenes, texto y datos de audio.\n",
    "- Crear perfiles de programas de **TensorFlow**.\n",
    "\n",
    "Una **Red Neuronal** decide cómo conectar las diferentes \"neuronas\" entre sus capas antes de que el modelo pueda predecir un resultado. Una vez que haya definido su arquitectura, no solo necesita entrenar el modelo sino también se requiere una métrica para calcular la precisión de la predicción, la cual se conoce como función de pérdida. El objetivo es minimizar la función de pérdida, esto es, hacer que el modelo cometa menos errores en sus predicciones. Para ello, el modelo repetirá muchas veces los cálculos hasta que la pérdida alcance una línea más plana. Para minimizar esta función de pérdida, es necesario definir una tasa de aprendizaje la cual es la velocidad con la que se desea que aprenda el modelo. \n",
    "\n",
    "**TensorBoard** es una herramienta de gran utilidad para visualizar dichas métricas y resaltar problemas potenciales. La **Red Neuronal** puede tomar mucho tiempo antes de encontrar una solución pero **TensorBoard** actualiza las métricas con mucha frecuencia y evita el esperar hasta el final del entrenamiento para ver si el modelo tiene un desempeño adecuado, ya que permite comprobar cómo va el entrenamiento y realizar el cambio adecuado si es necesario.\n",
    "\n",
    "Para ejecutar **TensorBoard** se ejecutan dos comandos mágicos ([*magic commands*](https://ipython.readthedocs.io/en/stable/interactive/magics.html)):\n",
    "- Se carga la extensión del *notebook* de **Tensorboard** por medio de `%load_ext tensorboard`. \n",
    "- Se inicializa **TensorBoard** dentro del *notebook* por medio de `%tensorboard --logdir logs/fit`.\n",
    "\n",
    "La documentación detallada de **TensorBoard** se puede revisar [a través de esta liga](https://www.tensorflow.org/tensorboard?hl=es-419)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extensión del Notebook de TensorBoard\n",
    "%load_ext tensorboard\n",
    "\n",
    "#Inicialización de TensorBoard en el Notebook\n",
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>.: Fin del Subtema :.</b>\n",
    "</div>\n",
    "\n",
    "***Liga de aceso al siguiente Subtema:*** \n",
    "<br>[e. Clasificación de Imágenes](e.%20Clasificacion%20de%20Imagenes.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
